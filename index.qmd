---
title: "Bayesian VARs Application: Forecast on Inflation"
author: "Xiaoman Guo"

execute:
  echo: false
  
bibliography: references.bib
---
> **Abstract.** This reseach project is motivated by the prevailing high inflation environment post pandemic, aiming to investigate and forecast how inflation will evolve over time under the applicaion of the Bayesian VARs.

> **Keywords.** Bayesian VARs, inflation, forecasting

## Objectives & Motivations

Since post pandemic, the inflation in Australia has increased largely, reaching at 7.8% per annual reported in the December quarter of 2022, becoming the highest rate in over 30 years from the speech of the [Australian Financial Review Business Summit](https://www.rba.gov.au/speeches/2023/sp-gov-2023-03-08.html#:~:text=In%20the%20December%20quarter%2C%20the,in%20more%20than%20three%20decades.). Given a high inflation can have negative impact in many aspects, such as reducing the consumer's purchasing power, and changing spending behaviour and investment decisions, it is not only a problem that every central bank is now aiming to deal with in order to achieve the price stability and the sustainable economic growth, but also can each one of us get to know how it will evolve in the future as it relates closely to our daily life.

The objective of this research project is to apply Bayesian VARs model to analyse the dynamic relationships of the inflation and three other economic variables so as to eventually forecast the time ahead inflation.

The questions to address is "what's the inflation in the next two years, and how soon the current inflation can return to the annual target, that is, 2 to 3 per cent in Australia?".

## Data & Data Properties

```{r library}
#| echo: false
#| message: false
#| warning: false
library(readabs)
library(readrba)
library(openxlsx)
library(lubridate)
library(TSstudio)
library(xts)
library(tseries)
library(mvtnorm)
```

The Consumer Price Index (CPI) is a well-known indicator of inflation, hence applied in the model to quantify it.

To forecast the inflation (or CPI), three other economic variables are proposed in the model for interaction, namely,

-   the Cash Rate of the Reserve Bank of Australia (RBA),

-   the Consumer Confidence Index from the Roy Morgan, and

-   the Wage Price Index from the Australian Bureau of Statistics (ABS).

Firstly, the Cash Rate is proposed given it's a means of the Monetary Policy having been used to control and target the inflation.

Secondly, according to the [RBA](https://www.rba.gov.au/education/resources/explainers/causes-of-inflation.html) , demand-pull, inflation expectations and cost-push are three broad categories causing the inflation. Therefore, the [Consumer Sentiment Index](https://melbourneinstitute.unimelb.edu.au/publications/macroeconomic-reports) indicating the consumer confidence level changes in the economic activity is proposed as a proxy measurement of the demand-pull and inflation expectation. In addition, the Wage Price Index measuring the total hourly pay rate changes for all Australian industries is proposed to analyse how inflation varies from the perspective of the cost-push.

The 4 variables are transformed into a consistent format for analysis.

```{r variables}
#| echo: false
#| message: false
#| warning: false

# Inflation CPI 
# Index Numbers ;  All groups CPI ;  Australia ; Sep-1948 to Mar-2023
cpi_dwnld   = read_abs(series_id = "A2325846C")     
cpi_tmp     = xts::xts(cpi_dwnld$value, cpi_dwnld$date)
cpi_tmp     = log(cpi_tmp)   # log transformation of the index value 

# Cash rate target
# Percentage, monthly ; Sep-1990 to Mar-2023
cashr_dwnld   = readrba::read_rba(series_id = "FIRMMCRT")   # Cash Rate Target
cashr_tmp     = xts::xts(cashr_dwnld$value, cashr_dwnld$date)
cashr_tmp     = apply.quarterly(cashr_tmp,mean) # aggregate monthly to quarterly average
cashr_tmp     = xts(cashr_tmp, seq(as.Date("1990-09-01"), by = "quarter", length.out = length(cashr_tmp)))

# Consumer confidence 
# Index value, quarterly ; Mar-2000 to Mar-2023
consum_dwnld  = read.xlsx("https://roymorgan-cms-dev.s3.ap-southeast-2.amazonaws.com/wp-content/uploads/2023/03/06052804/9180-ANZ-Roy-Morgan-Australian-CC-Data-1986-2023.xlsx",sheet=20)
consum_dwnld  = data.frame(na.omit(consum_dwnld[,4]))
consum_date   = seq(as.Date('2000-03-01'), as.Date('2023-03-01'), by = "quarter")
row.names(consum_dwnld) = consum_date
consum_tmp    = as.xts(log(consum_dwnld))
consum_tmp    = xts(consum_tmp, seq(as.Date('2000-03-01'), by = "quarter", length.out = length(consum_tmp)))

# Hourly wage 
# Total hourly rates of pay excluding bonuses ; Private and Public ;  All industries ; Sep-1997 - Dec-2022
wage_dwnld   = read_abs(series_id = "A2603609J")
wage_tmp     = xts::xts(wage_dwnld$value, wage_dwnld$date)
wage_tmp     = log(wage_tmp)   # log transformation of the index value 

# All Variables
variables_all = na.omit(merge(cpi_tmp ,cashr_tmp, consum_tmp, wage_tmp))
colnames(variables_all)   = c("CPI", "Cash Rate", "Consum Senti", "Hr Wage")
```

A time series plot is displayed below to visualize how the 4 proposed variables vary throughout the past 12 years starting from March 2011 to December 2022.

```{r plot}
#| echo: false
#| message: false
#| warning: false
#| 

#plot(variables_all[,1])
#plot(variables_all[,2])
#plot(variables_all[,3])
#plot(variables_all[,4])

ts_plot(na.omit(variables_all), type = "multiple", title = FALSE, Xgrid = TRUE)
```

The autocorrelation plot is presented below. It is clear that each variable shows some statistical significance in lag 1 indicating the random walk process.

```{r all variable acf}
#| echo: true
#| message: false
#| warning: false
acf(variables_all[,1])
acf(variables_all[,2])
acf(variables_all[,3])
acf(variables_all[,4])
```

A formal Augmented Dickey-Fuller (i.e. ADF) test is performed to test the stationarity of the variables.

```{r all variable ADF}
#| echo: true
#| message: false
#| warning: false
## Augmented Dickey-Fuller test
adf.results = matrix(NA, ncol(variables_all), 1)
for (i in (1:ncol(variables_all))){
 adf = adf.test(variables_all[,i], k=4)
 adf.results[i] = round(adf$p.value,2)
}
colnames(adf.results) = c("p.value")
rownames(adf.results) = c("CPI", "Cash Rate", "Consum Senti", "Hr Wage")
adf.results
```

According to the ADF test results, we do not reject the null hypothesis of non-stationary, and thus conclude the chosen variables show some stochastic random walk process. This finding supports the specification of the prior distribution we will discuss later.

## Model and Hypotheses

The basic model equation we will build upon throughout the research project is stated below with the error term specified in the matrix-variate normal distribution, which includes a T x 4 dimension mean, a 4 x 4 dimension row specific covariance matrix and a T dimension identity matrix referring to the column specified covariance.

-   Y is a T x 4 matrix representing the 4 variables introduced in the model as explained previously and each variable spreading out to T periods.

-   X is a T x (1+4P) matrix with T referring to the periods of time and (1+4P) referring to the 4 variables, each of which has p lags in the VAR system.

-   A is a (1+4P) x 4 matrix with 1+4P corresponding to every coefficient in the referring variable.

-   E is a T x 4 matrix referring to the error term in each of the equation.

```{=tex}
\begin{align}
Y &= XA + E \\
\end{align}
```
```{=tex}
\begin{align}
E |X  &\sim MN_{T \times 4 } (\textbf{0}_{T\times 4}, \Sigma , I_{T}) \\
\end{align}
```
## Modelling Framework

### Basic Model

The basic model is built on the natural-conjugate prior distribution, which is specified as a matrix normal inverse Wishart distribution. Miranda-Agrippino and Ricco (2018) assert that "in scientific data analysis, priors on the model coefficients do not incorporate the investigator's 'subjective' beliefs, instead, they summarise stylised representations of the data generating process". Therefore, Minnesota prior expressing such stylised facts, e.g. unit root non-stationary, discovered in the macroeconomic time series is applied to form the specifications on the parameters of the prior distribution.

Below presents the estimation procedures to draw posterior samples.

**Step 1**: Prior distribution is presented below. We will specify $\underline{A}$, $\underline{V}$, $\underline{S}$ and $\underline{v}$.

```{=tex}
\begin{align}
p(A,\Sigma) &= p(A|\Sigma)p(\Sigma)\\
A|\Sigma &\sim MN_{K\times N}(\underline{A}, \Sigma,\underline{V} ) \\ 
\Sigma &\sim IW_{N}(\underline{S}, \underline{v})
\end{align}
```
-   $\underline{A}$ is a $K \times N$ matrix being set to reflect the random walk with no drift process with the coefficients being 1 on its own lag 1, and 0 on other lags.

```{=tex}
\begin{align}
\underline{A} = \left[ \underbrace{0_{N\times 1}}_{intercept} \quad \underbrace{I_{N}}_{A_{1}}  \quad\underbrace{0_{N\times (p-1)N}}_{A_{2}  - A_{p} } \right]
\end{align}
```
-   $\underline{V}$ represents the shrinking level of the specified $\underline{A}$. It's a $K$ vector diagonal matrix with the diagonal elements set to be the desired shrinking amount, the larger the figure, the looser the shrinkage meaning larger variance is allowed; and off-diagonal being 0 as less information can be known about the covariances among coefficients.

```{=tex}
\begin{align}
\underline{V} = diag\left[ \underbrace{k_{2}}_{intercept} \quad \underbrace{k_{1}(p^{-2}\otimes l^{'}_{N})}_{A_{1} \; to\;A_{p}}  \right]
\end{align}
```
-   $\underline{S}$ follows the econometrics convention to set as a $N$ vector diagonal matrix with the estimated $\widehat{\sigma}^{2}$ of each variable being the diagonal elements.

-   $\underline{v}$ is $N+1$.

**Step 2**: Given the posterior distribution is also a matrix normal inverse Wishart distribution and its parameters are composed of data and prior parameters. We can insert parameters as specified in step 1 in the below equations.

```{=tex}
\begin{align}
p(A,\Sigma | Y,X) &= p(A|Y,X,\Sigma)p(\Sigma|Y,X)\\
p(A|Y,X,\Sigma)&\sim MN_{K\times N}(\overline{A}, \Sigma,\overline{V} ) \\ 
p(\Sigma|Y,X)&\sim IW_{N}(\overline{S}, \overline{v})
\end{align}
```

```{=tex}
\begin{align}
\overline{V} &= (X'X + \underline{V}^{-1})^{-1} \\
\overline{A} &= \overline{V}(X'Y+\underline{V}^{-1}\underline{A}) \\
\overline{v} &= T + \underline{v} \\ 
\overline{S} &= \underline{S}+Y'Y+\underline{A'}\underline{V}^{-1}\underline{A}-\overline{A'}\overline{V}^{-1}\overline{A} \\
\end{align}
```
**Step 3**: Given $\overline{A}$, $\overline{V}$, $\overline{S}$ and $\overline{v}$ are now specified, we can firstly draw $\Sigma$ from $IW_{N}(\overline{S}, \overline{v})$, then take $\Sigma$ as known and insert it in $MN_{K\times N}(\overline{A}, \Sigma,\overline{V} )$ to draw $A$.

The above steps are summarised in the below code to generate the sample draws from the joint posterior distribution.

```{r function based on basic model}
#| echo: true
#| message: false
#| warning: false
## Present data X, Y
y = ts(variables_all[,1:4])
Y = ts(y[5:nrow(y),], frequency=4)
X = matrix(1,nrow(Y),1)
for (i in 1:frequency(Y)){
  X     = cbind(X,y[5:nrow(y)-i,])
}
 
## Pre-setup 
N           = ncol(Y)
p           = frequency(Y)
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/T 

# Prior distribution specification - Minnesota prior 
kappa.1     = 0.02^2                              # shrinkage for A1 to Ap
kappa.2     = 100                                 # shrinkage for constant 
A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N + 1),] = diag(N)
V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     = diag(diag(Sigma.hat))
nu.prior    = N+1

## Posterior sample draw function 
posterior.draws = function (S, Y, X){
    # normal-inverse Wishard posterior parameters
    V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))
    V.bar       = solve(V.bar.inv)
    A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar      = nrow(Y) + nu.prior
    S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv   = solve(S.bar)
  
    # posterior draws 
    Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior   = apply(Sigma.posterior,3,solve)
    Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
    A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
    L                 = t(chol(V.bar))
    for (s in 1:S){
      A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    }
 
    output      = list(A.posterior=A.posterior, Sigma.posterior=Sigma.posterior)
    return(output)
}

## Applying function 
# A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
posterior.sample.draws = posterior.draws(S=50000, Y=Y, X=X)
round(apply(posterior.sample.draws$A.posterior, 1:2, mean),6)
round(apply(posterior.sample.draws$Sigma.posterior, 1:2, mean),6)
```

### Model Extension

The model extension applied in this research report is to build hierarchical model by imposing the inverse gamma 2 distribution on the Minnesota shrinkage parameter kappa $k$. After specifying $k$, we can write the posterior distribution as below.

```{=tex}
\begin{align}
p(A,\Sigma, k |Y,X) &\propto L(Y,X|A,\Sigma)p(A,\Sigma, k)\\
&\propto L(Y,X|A,\Sigma)p(A |\Sigma, k)p(\Sigma)p(k)
\end{align}
```
where, each $p(A |\Sigma, k)$, $p(\Sigma)$, $p(k)$ is specified below.

```{=tex}
\begin{align}
p(A |\Sigma, k) &\sim MN_{K\times N}(\underline{A}, \Sigma, k\underline{V})\\
p(\Sigma) &\sim IW_{N}(\underline{S},\underline{v})\\
p(k) &\sim IG2(\underline{S_{k}}, \underline{v_{k}} ) \\ 
\end{align}
```
We multiply the corresponding distribution probability density functions (i.e. pdf) of $L(Y,X|A,\Sigma)p(A|\Sigma, k)p(\Sigma)p(k)$, the kennel of the posterior distribution is written as below.

```{=tex}
\begin{align}
p(A,\Sigma, k |Y,X) &\propto \det(\Sigma)^{-\frac{T+N+K+\underline{v}+1}{2}} \\
&\times \exp\left\{ -\frac{1}{2} TR[\Sigma^{-1}[(A-\overline{A})'\overline{V}^{-1}(A-\overline{A})+\underline{S}+Y'Y+\underline{A}'(k\underline{V})^{-1}\underline{A}-\overline{A}'\overline{V}^{-1}\overline{A}]  ] \right\} \\
&\times k^{-\frac{\underline{v_{k}}+2}{2}} \exp\left\{ -\frac{1}{2}\frac{\underline{S_{k}}}{k} \right\}\\
\end{align}
```
The derivation is performed on each posterior parameter. The resulting equation is presented below.

```{=tex}
\begin{align}
p(A |Y,X,\Sigma, k) &\sim MN_{K\times N}(\overline{A}, \Sigma, \overline{V})\\
p(\Sigma|Y,X,A,k) &\sim IW_{N}(\overline{S},\overline{v})\\

\overline{V} &= (X'X + (k\underline{V})^{-1})^{-1}\\
\overline{A} &= \overline{V}(X'Y+(k\underline{V})^{-1}\underline{A}) \\
\overline{v} &= T+\underline{v}\\
\overline{S} &= \underline{S}+Y'Y+\underline{A}'(k\underline{V})^{-1}\underline{A}-\overline{A}'\overline{V}^{-1}\overline{A} \\
\end{align}
```

```{=tex}
\begin{align}
p(k |Y,X, A,\Sigma) &\sim IG2(\overline{S_{k}}, \overline{v_{k}} ) \\ 
\overline{v_{k}} &= \underline{v_{k}}\\
\overline{S_{k}} &= TR(\Sigma^{-1}\underline{A}'(\underline{V})^{-1}\underline{A}) + \underline{S_{k}}\\
\end{align}
```

Since we don't have an analytical derivation of the joint posterior distribution $p(A,\Sigma, k |Y,X)$, the Gibbs sampler method is applied to generate random draws based on the full conditional posterior distribution of $A$, $\Sigma$ and $k$ as presented above. The steps of the sample draws are explained below.

Initialize $k$ at $k^{(0)}$.

At each iteration $s$:

1.  Draw random matrices for $A^{(s)}$ and $\Sigma^{(s)}$ from $p(A,\Sigma|Y,X,k^{(s-1)})$.

2.  Draw a random number for $k^{(s)}$ from $p(k |Y,X,A^{(s)},\Sigma^{(s)})$.

Repeat steps 1 and 2 $S_{1} + S_{2}$ times.

Discard the first $S_{1}$ draws that allows the algorithm to converge to the stationary posterior distribution.

Output is the sample draws from the joint posterior distribution $\left\{ {A^{(s)}, \Sigma^{(s)}, k^{(s)}} \right\}^{S_{2}}_{s=S_{1}+1}$.

The above steps are summarised in the below code to generate the sample draws from the joint posterior distribution.

```{r function based on extended model (1)}
#| echo: false
#| message: false
#| warning: false

## Present data X, Y
y       = ts(variables_all[,1:4])
Y       = ts(y[5:nrow(y),], frequency=4)
X       = matrix(1,nrow(Y),1)
for (i in 1:frequency(Y)){
  X     = cbind(X,y[5:nrow(y)-i,])
}
 
## Pre-setup 
N           = ncol(Y)
p           = frequency(Y)
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/T 

# Prior distribution specification - Minnesota prior 
kappa.1     = 0.02^2                                # shrinkage for A1 to Ap
kappa.2     = 100                                   # shrinkage for constant 
A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N + 1),] = diag(N)
V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     = diag(diag(Sigma.hat))
nu.prior    = N+1
```

```{r function based on extended model}
#| echo: true
#| message: false
#| warning: false
# setup 
S1          = 100                              # determine the burn-in draws
S2          = 1000                             # number of draws from the final simulation
total_S     = S1+S2
A.posterior       = array(NA, dim = c((1+N*p),N,S1+S2))
Sigma.posterior   = array(NA, dim = c(N,N,S1+S2))
k.posterior       = matrix(NA, S1+S2, 1)

k.posterior[1]    = 1                          # set k0 

# Prior IG2 distribution: kappa
S.k.prior   = 1
nu.k.prior  = 1

## Posterior sample draw function for extended model  
posterior.draws.exten = function (total_S, Y, X){
for (s in 1:total_S){
    # normal-inverse Wishard posterior parameters
    V.bar.inv              = t(X)%*%X + diag(1/ diag( k.posterior[s]* V.prior))
    V.bar                  = solve(V.bar.inv)
    A.bar                  = V.bar%*%(t(X)%*%Y + diag(1/diag( k.posterior[s]* V.prior))%*%A.prior)
    nu.bar                 = nrow(Y) + nu.prior
    S.bar                  = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag( k.posterior[s]* V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv              = solve(S.bar)
  
    # posterior draws for A and Sigma
    Sigma.posterior.IW     = rWishart(1, df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior.draw   = apply(Sigma.posterior.IW,3,solve)
    Sigma.posterior[,,s]   = Sigma.posterior.draw
    A.posterior[,,s]       = array(rnorm(prod(c(dim(A.bar),1))),c(dim(A.bar),1))
    L                      = t(chol(V.bar))
    A.posterior[,,s]       = A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    
    # posterior draws for k
    if (s!=total_S){
    S.k.bar                = sum(diag( solve( Sigma.posterior[,,s] ) * t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior )) + S.k.prior
    nu.k.bar               = nu.k.prior 
    k.draw.tmp             = rchisq(1, df=nu.k.bar)
    k.draw                 = S.k.bar/k.draw.tmp
    k.posterior[s+1]       = k.draw
  }
}
    output                 = list (A.posterior.exten = A.posterior, Sigma.posterior.exten = Sigma.posterior, k.posterior.exten = k.posterior)
    return(output)
}
  
## Applying function 
posterior.ext = posterior.draws.exten(total_S = total_S, Y=Y, X=X)
round(apply(posterior.ext$A.posterior.exten[,,(S1+1):S2], 1:2, mean),6)
round(apply(posterior.ext$Sigma.posterior.exten[,,(S1+1):S2], 1:2, mean),6)

```

### Model Proof
```{r function replication 1}
#| echo: true
#| message: false
#| warning: false
test_sigma = diag(1,2)
m = rmvnorm(n=1000, mean=c(0,0), sigma=test_sigma)

## Define data X, Y 
Y = ts(m[2:nrow(m),], frequency=1)
X = matrix(1,nrow(Y),1)
X = cbind(X,m[2:nrow(m)-1,])

## Test on basic model
N           = ncol(Y)
p           = frequency(Y)
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/T

# Prior distribution specification - Minnesota prior 
kappa.1     = 0.02^2                               # shrinkage for A1 to Ap
kappa.2     = 100                                  # shrinkage for constant 
A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N + 1),] = diag(N)
V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     = diag(diag(Sigma.hat))
nu.prior    = N+1

# Applying function 
posterior.sample.draws = posterior.draws(S=100000, Y=Y, X=X)
round(apply(posterior.sample.draws$A.posterior, 1:2, mean),6)
round(apply(posterior.sample.draws$Sigma.posterior, 1:2, mean),6)

# setup 
S1          = 100                              # determine the burn-in draws
S2          = 1000                             # number of draws from the final simulation
total_S     = S1+S2
A.posterior       = array(NA, dim = c((1+N*p),N,S1+S2))
Sigma.posterior   = array(NA, dim = c(N,N,S1+S2))
k.posterior       = matrix(NA, S1+S2, 1)
k.posterior[1]    = 1                          # set k0 

# Prior IG2 distribution: kappa
S.k.prior   = 1
nu.k.prior  = 1

# Applying function 
posterior.ext = posterior.draws.exten(total_S = total_S, Y=Y, X=X)
round(apply(posterior.ext$A.posterior.exten[,,(S1+1):S2], 1:2, mean),6)
round(apply(posterior.ext$Sigma.posterior.exten[,,(S1+1):S2], 1:2, mean),6)
```


## References {.unnumbered}